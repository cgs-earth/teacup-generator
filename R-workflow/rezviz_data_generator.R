# rezviz_data_generator.R
#
# DAILY SCRIPT: Query current reservoir conditions, combine with historical
# statistics, generate output CSV for teacup visualization, and upload to
# HydroShare.
#
# Designed to run daily via cron/scheduler.
# Depends on historical_statistics.parquet created by setup_historical_baseline.R
#
# Fetches data from multiple sources:
#   - RISE via WWDH EDR API (majority of locations)
#   - USACE CDA API (Cochiti, Abiquiu, Santa Rosa, Grand Coulee, Fort Peck, Lucky Peak)
#   - USGS Water Data OGC API (Lahontan, Boca, Prosser Creek, Stampede, Upper Klamath)
#   - CDEC (Tahoe)
#
# HydroShare credentials loaded from .env file in the working directory.
# Expected .env format:
#   HYDROSHARE_USERNAME=user@example.com
#   HYDROSHARE_PASSWORD=yourpassword
#
# Author: Kyle Onda, CGS
# Created: 2026-01-28
################################################################################

library(httr2)
library(dplyr)
library(readr)
library(lubridate)
library(arrow)
library(stringr)
library(sf)
library(jsonlite)

# Load .env file if present
if (file.exists(".env")) {
  readRenviron(".env")
}

################################################################################
# CONFIGURATION
################################################################################

# API base URL for RISE data via WWDH
WWDH_API_BASE <- "https://api.wwdh.internetofwater.app"

# HydroShare
HYDROSHARE_RESOURCE_ID <- "22b2f10103e5426a837defc00927afbd"
HYDROSHARE_BASE_URL <- "https://www.hydroshare.org"

# Paths
OUTPUT_DIR       <- "output"
CONFIG_DIR       <- "config"
HYDROSHARE_DIR   <- "hydroshare"

# Target date (default: yesterday, can override via command line)
args <- commandArgs(trailingOnly = TRUE)
if (length(args) > 0) {
  TARGET_DATE <- as.Date(args[1])
} else {
  TARGET_DATE <- Sys.Date() - 1
}

# How many days to look back if no data on target date
LOOKBACK_DAYS <- 7

# Historical statistics period
STATS_PERIOD <- "10/1/1990 - 9/30/2020"

message(sprintf("=== Reservoir Data Generator ==="))
message(sprintf("Target date: %s", TARGET_DATE))
message(sprintf("Run time: %s", Sys.time()))

################################################################################
# PARQUET FILE PATHS
################################################################################

# Parquet files should be bundled in the Docker image or present locally.
# After backfill, updated parquet files are uploaded to HydroShare.
# Rebuild the Docker image periodically to incorporate backfills.

stats_file <- file.path(OUTPUT_DIR, "historical_statistics.parquet")
baseline_file <- file.path(OUTPUT_DIR, "historical_baseline.parquet")

################################################################################
# LOAD HISTORICAL STATISTICS
################################################################################

if (!file.exists(stats_file)) {
  stop("Historical statistics file not found. Run setup_historical_baseline.R first.")
}

historical_stats <- read_parquet(stats_file)
message(sprintf("Loaded historical statistics: %d rows for %d locations",
                nrow(historical_stats),
                n_distinct(historical_stats$location_id)))

# Filter out locations whose historical baseline does not adequately cover the
# full 30-water-year period (Oct 1990 â€“ Sep 2020).  We require at least 20
# water years of observations; locations with fewer have a systematically
# incomplete period of record and their percentiles/means are unreliable.
MIN_WATER_YEARS <- 20

baseline_file <- file.path(OUTPUT_DIR, "historical_baseline.parquet")
if (file.exists(baseline_file)) {
  baseline <- read_parquet(baseline_file)
  wy_coverage <- baseline |>
    mutate(water_year = ifelse(month(date) >= 10, year(date) + 1, year(date))) |>
    group_by(location_id) |>
    summarize(n_water_years = n_distinct(water_year), .groups = "drop")

  inadequate <- wy_coverage |>
    filter(n_water_years < MIN_WATER_YEARS) |>
    pull(location_id)

  if (length(inadequate) > 0) {
    message(sprintf("  Excluding %d locations with < %d water years of baseline data:",
                    length(inadequate), MIN_WATER_YEARS))
    for (loc_id in inadequate) {
      nwy <- wy_coverage$n_water_years[wy_coverage$location_id == loc_id]
      message(sprintf("    %s (%d water years)", loc_id, nwy))
    }
    historical_stats <- historical_stats |>
      filter(!location_id %in% inadequate)
    message(sprintf("  Retained historical statistics for %d locations",
                    n_distinct(historical_stats$location_id)))
  }
  rm(baseline, wy_coverage, inadequate)
} else {
  message("  WARNING: historical_baseline.parquet not found; cannot verify coverage.")
}

################################################################################
# LOAD LOCATION METADATA
################################################################################

locations_file <- file.path(CONFIG_DIR, "locations.geojson")
locations_sf <- st_read(locations_file, quiet = TRUE)

# Extract location metadata from geojson - ALL locations
locations <- locations_sf |>
  st_drop_geometry() |>
  transmute(
    name = Name,
    location_id = Identifier,
    capacity = as.numeric(str_remove_all(`Total.Capacity`, ",")),
    active_capacity = as.numeric(str_remove_all(`Active.Capacity`, ",")),
    label_map = `Preferred.Label.for.Map.and.Table`,
    label_popup = `Preferred.Label.for.PopUp.and.Modal`,
    state = state,
    doi_region = doiRegion,
    huc6 = huc6,
    longitude = Longitude,
    latitude = Latitude,
    source = `Source.for.Storage.Data`,
    data_type = `Storage.Data.Type`  # "Storage" or "Elevation"
  )

message(sprintf("Loaded %d locations from geojson", nrow(locations)))

################################################################################
# ELEVATION-TO-STORAGE CONVERSION
################################################################################

# Load elevation-storage curves for locations that report elevation instead of storage
elev_curves_file <- file.path(CONFIG_DIR, "elevation_storage_curves.csv")
if (file.exists(elev_curves_file)) {
  elev_curves <- read_csv(elev_curves_file, comment = "#", show_col_types = FALSE)
  elev_curve_ids <- unique(elev_curves$location_id)
  message(sprintf("Loaded elevation-storage curves for %d location(s): %s",
                  length(elev_curve_ids), paste(elev_curve_ids, collapse = ", ")))
} else {
  elev_curves <- NULL
  elev_curve_ids <- character(0)
  message("No elevation-storage curves file found")
}

#' Convert elevation to storage using linear interpolation
#'
#' @param location_id The location identifier
#' @param elevation_ft Water surface elevation in feet
#' @return Storage in acre-feet, or NA if no curve available
elevation_to_storage <- function(location_id, elevation_ft) {
  if (is.null(elev_curves) || is.na(elevation_ft)) return(NA_real_)

  loc_id <- as.character(location_id)
  curve <- elev_curves |> filter(location_id == loc_id)

  if (nrow(curve) == 0) {
    warning(sprintf("No elevation-storage curve for location %s", loc_id))
    return(NA_real_)
  }

  # Handle edge cases
  if (elevation_ft <= min(curve$elevation_ft)) return(min(curve$storage_af))
  if (elevation_ft >= max(curve$elevation_ft)) return(max(curve$storage_af))

  # Linear interpolation
  # Find the two points to interpolate between
  curve <- curve |> arrange(elevation_ft)
  idx_upper <- which(curve$elevation_ft >= elevation_ft)[1]
  idx_lower <- idx_upper - 1

  x1 <- curve$elevation_ft[idx_lower]
  x2 <- curve$elevation_ft[idx_upper]
  y1 <- curve$storage_af[idx_lower]
  y2 <- curve$storage_af[idx_upper]

  # Linear interpolation formula
  storage <- y1 + (elevation_ft - x1) * (y2 - y1) / (x2 - x1)
  return(storage)
}

# Check for locations that need elevation conversion but lack curves
elevation_locations <- locations |> filter(tolower(data_type) == "elevation")
if (nrow(elevation_locations) > 0) {
  missing_curves <- setdiff(elevation_locations$location_id, elev_curve_ids)
  if (length(missing_curves) > 0) {
    message(sprintf("WARNING: %d location(s) report elevation but lack conversion curves:",
                    length(missing_curves)))
    for (loc_id in missing_curves) {
      loc_name <- locations$name[locations$location_id == loc_id]
      message(sprintf("  - %s (ID: %s)", loc_name, loc_id))
    }
  }
}

################################################################################
# SOURCE TYPE CLASSIFICATION
################################################################################

#' Classify a location's data source type from the Source field in geojson
#'
#' @param source_str The "Source for Storage Data" field value
#' @return One of: "rise", "usace_cda", "usgs", "cdec", "unknown"
classify_source <- function(source_str) {
  if (is.na(source_str) || source_str == "") return("unknown")
  s <- tolower(source_str)
  if (str_detect(s, "^rise"))                                  return("rise")
  if (str_detect(s, "usace") || str_detect(s, "water\\.usace")) return("usace_cda")
  if (str_detect(s, "^usgs") || str_detect(s, "waterdata\\.usgs")) return("usgs")
  if (str_detect(s, "cdec\\.water\\.ca\\.gov"))                return("cdec")
  return("unknown")
}

# Note: All data sources now use the geojson Identifier directly:
#   - RISE: Identifier is the RISE location ID (e.g., "7166")
#   - USACE: Identifier is "provider/ts_name" (e.g., "spa/Abiquiu.Stor.Inst.15Minutes.0.DCP-rev")
#   - USGS: Identifier is the USGS site number (e.g., "10344490")
#   - CDEC: Identifier is the CDEC station code (e.g., "THC")

################################################################################
# DETECT AND BACKFILL NEW LOCATIONS
################################################################################

# Check which locations are missing from historical baseline
# This catches both:
#   1. Newly added locations (not previously in locations.csv)
#   2. Status changes from "Do Not Include" -> "Include" (now in geojson but no baseline data)
#
# The geojson only contains "Include" locations, so any location_id in geojson
# but not in historical_baseline.parquet needs to be backfilled.

baseline_file <- file.path(OUTPUT_DIR, "historical_baseline.parquet")
baseline_location_ids <- if (file.exists(baseline_file)) {
  read_parquet(baseline_file) |> pull(location_id) |> unique()
} else {
  character(0)
}

new_location_ids <- setdiff(locations$location_id, baseline_location_ids)
new_locations <- locations |> filter(location_id %in% new_location_ids)

if (nrow(new_locations) > 0) {

  message(sprintf("\n=== Detected %d new location(s) requiring backfill ===", nrow(new_locations)))
  message("(These may be newly added or changed from 'Do Not Include' to 'Include')\n")
  for (j in seq_len(nrow(new_locations))) {
    message(sprintf("  - %s (ID: %s)", new_locations$name[j], new_locations$location_id[j]))
  }
  message("")

  # Historical period for baseline
  BASELINE_START <- as.Date("1990-10-01")
  BASELINE_END   <- as.Date("2020-09-30")

  new_baseline_data <- list()

  for (i in seq_len(nrow(new_locations))) {
    loc <- new_locations[i, ]
    loc_id        <- loc$location_id
    loc_name      <- loc$name
    source_str    <- loc$source
    data_type_str <- if (!is.na(loc$data_type)) loc$data_type else "Storage"
    src_type      <- classify_source(source_str)

    type_suffix <- if (tolower(data_type_str) == "elevation") " (elevation->storage)" else ""
    message(sprintf("[%d/%d] Backfilling %s (ID: %s) [%s]%s...",
                    i, nrow(new_locations), loc_name, loc_id, src_type, type_suffix))

    # Fetch full historical range based on source type
    hist_data <- NULL

    if (src_type == "rise") {
      # RISE: fetch full range via WWDH API
      url <- paste0(
        WWDH_API_BASE,
        "/collections/rise-edr/locations/", loc_id,
        "?parameter-name=Storage",
        "&limit=50000",
        "&datetime=", BASELINE_START, "/", BASELINE_END + 1,
        "&f=csv"
      )

      tryCatch({
        response <- request(url) |>
          req_timeout(300) |>
          req_retry(max_tries = 3, backoff = ~ 10) |>
          req_perform()

        if (resp_status(response) == 200) {
          csv_content <- resp_body_string(response)
          if (nchar(csv_content) > 50) {
            data <- read_csv(I(csv_content), show_col_types = FALSE)
            if (nrow(data) > 0 && "datetime" %in% names(data)) {
              hist_data <- data |>
                transmute(
                  location_id = loc_id,
                  date = as.Date(datetime),
                  value = value,
                  unit = unit
                ) |>
                filter(!is.na(value)) |>
                distinct(date, .keep_all = TRUE)
            }
          }
        }
      }, error = function(e) {
        message(sprintf("    Error fetching RISE historical: %s", conditionMessage(e)))
      })

    } else if (src_type == "usace_cda") {
      # USACE: parse provider/ts_name and fetch
      id_str <- as.character(loc_id)
      slash_pos <- str_locate(id_str, "/")[1, "start"]

      if (!is.na(slash_pos)) {
        provider <- str_sub(id_str, 1, slash_pos - 1)
        ts_name  <- str_sub(id_str, slash_pos + 1)

        begin_str <- paste0(format(BASELINE_START, "%Y-%m-%dT00:00:00"), ".000Z")
        end_str   <- paste0(format(BASELINE_END + 1, "%Y-%m-%dT00:00:00"), ".000Z")

        url <- sprintf(
          "https://water.usace.army.mil/cda/reporting/providers/%s/timeseries?name=%s&begin=%s&end=%s&format=csv",
          provider, URLencode(ts_name, reserved = TRUE), begin_str, end_str
        )

        tryCatch({
          response <- request(url) |>
            req_timeout(300) |>
            req_retry(max_tries = 3, backoff = ~ 10) |>
            req_perform()

          body <- resp_body_string(response)
          body <- str_replace_all(body, "\r", "")
          lines <- str_split(body, "\n")[[1]]

          unit_line <- lines[str_starts(lines, "##unit:")]
          unit_val <- if (length(unit_line) > 0) trimws(str_remove(unit_line[1], "##unit:")) else "ac-ft"
          if (tolower(unit_val) == "ac-ft") unit_val <- "af"

          data_lines <- lines[!str_starts(lines, "##") & nchar(trimws(lines)) > 0]

          if (length(data_lines) > 0) {
            hist_data <- tibble(raw = data_lines) |>
              mutate(
                datetime = str_extract(raw, "^[^,]+"),
                value = as.numeric(str_extract(raw, "[^,]+$")),
                date = as.Date(str_sub(datetime, 1, 10))
              ) |>
              filter(!is.na(value), !is.na(date)) |>
              group_by(date) |>
              summarize(value = last(value), .groups = "drop") |>
              mutate(location_id = loc_id, unit = unit_val) |>
              select(location_id, date, value, unit)
          }
        }, error = function(e) {
          message(sprintf("    Error fetching USACE historical: %s", conditionMessage(e)))
        })
      }

    } else if (src_type == "usgs") {
      # USGS: fetch via OGC API
      # Handle elevation vs storage data types
      site_no <- as.character(loc_id)

      # Select parameter code based on data type
      if (tolower(data_type_str) == "elevation") {
        param_code <- "62614"  # Elevation (NGVD29)
      } else {
        param_code <- "00054"  # Storage (acre-feet)
      }

      url <- sprintf(
        "https://api.waterdata.usgs.gov/ogcapi/v0/collections/daily/items?f=json&monitoring_location_id=USGS-%s&parameter_code=%s&time=%s/%s&limit=50000",
        site_no, param_code, BASELINE_START, BASELINE_END
      )

      tryCatch({
        response <- request(url) |>
          req_timeout(300) |>
          req_retry(max_tries = 3, backoff = ~ 10) |>
          req_perform()

        body <- resp_body_string(response)
        data <- jsonlite::fromJSON(body, simplifyVector = FALSE)

        features <- data$features

        # If elevation query returned nothing, try alternate elevation parameters
        # Priority: 72275 (USBR datum, e.g. Klamath Basin) > 62615 (NAVD88) > 62614 (NGVD29)
        if (length(features) == 0 && tolower(data_type_str) == "elevation") {
          for (alt_param in c("72275", "62615")) {
            message(sprintf("    Trying alternate elevation parameter %s...", alt_param))
            url <- sprintf(
              "https://api.waterdata.usgs.gov/ogcapi/v0/collections/daily/items?f=json&monitoring_location_id=USGS-%s&parameter_code=%s&time=%s/%s&limit=50000",
              site_no, alt_param, BASELINE_START, BASELINE_END
            )
            response <- request(url) |>
              req_timeout(300) |>
              req_retry(max_tries = 3, backoff = ~ 10) |>
              req_perform()
            body <- resp_body_string(response)
            data <- jsonlite::fromJSON(body, simplifyVector = FALSE)
            features <- data$features
            if (length(features) > 0) {
              message(sprintf("    Found %d records with parameter %s", length(features), alt_param))
              break
            }
          }
        }

        if (length(features) > 0) {
          hist_data <- tibble(
            location_id = loc_id,
            date = as.Date(sapply(features, function(f) f$properties$time)),
            value = as.numeric(sapply(features, function(f) f$properties$value)),
            unit = sapply(features, function(f) f$properties$unit_of_measure)
          ) |>
            filter(!is.na(value)) |>
            distinct(date, .keep_all = TRUE)

          # Convert elevation to storage if needed
          if (tolower(data_type_str) == "elevation") {
            message(sprintf("    Converting %d elevation readings to storage...", nrow(hist_data)))
            hist_data <- hist_data |>
              rowwise() |>
              mutate(
                storage = elevation_to_storage(location_id, value)
              ) |>
              ungroup() |>
              filter(!is.na(storage)) |>
              mutate(
                value = storage,
                unit = "af"
              ) |>
              select(-storage)
            message(sprintf("    Successfully converted %d readings", nrow(hist_data)))
          } else {
            hist_data <- hist_data |>
              mutate(unit = ifelse(tolower(unit) == "acre-ft", "af", tolower(unit)))
          }
        }
      }, error = function(e) {
        message(sprintf("    Error fetching USGS historical: %s", conditionMessage(e)))
      })

    } else if (src_type == "cdec") {
      # CDEC: fetch via CSV servlet
      station <- as.character(loc_id)
      url <- sprintf(
        "https://cdec.water.ca.gov/dynamicapp/req/CSVDataServlet?Stations=%s&SensorNums=15&dur_code=D&Start=%s&End=%s",
        station, BASELINE_START, BASELINE_END
      )

      tryCatch({
        response <- request(url) |>
          req_timeout(300) |>
          req_retry(max_tries = 3, backoff = ~ 10) |>
          req_perform()

        body <- resp_body_string(response)
        data <- read_csv(I(body), show_col_types = FALSE,
                         col_types = cols(`DATE TIME` = col_character(),
                                          `OBS DATE` = col_character(),
                                          .default = col_guess()))

        if (nrow(data) > 0 && "VALUE" %in% names(data)) {
          unit_val <- if ("UNITS" %in% names(data)) tolower(data$UNITS[1]) else "af"

          hist_data <- data |>
            mutate(
              date = as.Date(str_sub(`DATE TIME`, 1, 8), format = "%Y%m%d"),
              value = as.numeric(VALUE)
            ) |>
            filter(!is.na(value)) |>
            transmute(location_id = loc_id, date, value, unit = unit_val) |>
            distinct(date, .keep_all = TRUE)
        }
      }, error = function(e) {
        message(sprintf("    Error fetching CDEC historical: %s", conditionMessage(e)))
      })
    }

    if (!is.null(hist_data) && nrow(hist_data) > 0) {
      message(sprintf("    Retrieved %d historical observations", nrow(hist_data)))
      new_baseline_data[[length(new_baseline_data) + 1]] <- hist_data
    } else {
      message(sprintf("    No historical data retrieved"))
    }

    Sys.sleep(1)  # Rate limiting for bulk fetches
  }

  # Combine and append to baseline
  if (length(new_baseline_data) > 0) {
    new_baseline <- bind_rows(new_baseline_data)
    message(sprintf("\nTotal new historical observations: %d", nrow(new_baseline)))

    # Load existing baseline and append
    if (file.exists(baseline_file)) {
      existing_baseline <- read_parquet(baseline_file)
      combined_baseline <- bind_rows(existing_baseline, new_baseline)
    } else {
      combined_baseline <- new_baseline
    }

    # Save updated baseline
    write_parquet(combined_baseline, baseline_file)
    message(sprintf("Updated historical_baseline.parquet: %d total observations",
                    nrow(combined_baseline)))

    # Compute statistics for new locations
    message("\nComputing statistics for new locations...")

    new_stats <- new_baseline |>
      filter(date >= BASELINE_START, date <= BASELINE_END) |>
      mutate(month = month(date), day = day(date)) |>
      group_by(location_id, month, day) |>
      summarize(
        min  = min(value, na.rm = TRUE),
        max  = max(value, na.rm = TRUE),
        mean = mean(value, na.rm = TRUE),
        p10  = quantile(value, 0.10, na.rm = TRUE),
        p25  = quantile(value, 0.25, na.rm = TRUE),
        p50  = quantile(value, 0.50, na.rm = TRUE),
        p75  = quantile(value, 0.75, na.rm = TRUE),
        p90  = quantile(value, 0.90, na.rm = TRUE),
        n    = n(),
        unit = first(unit),
        .groups = "drop"
      )

    # Append to historical statistics
    stats_file <- file.path(OUTPUT_DIR, "historical_statistics.parquet")
    if (file.exists(stats_file)) {
      existing_stats <- read_parquet(stats_file)
      combined_stats <- bind_rows(existing_stats, new_stats)
    } else {
      combined_stats <- new_stats
    }

    write_parquet(combined_stats, stats_file)
    message(sprintf("Updated historical_statistics.parquet: %d total rows",
                    nrow(combined_stats)))

    # Reload historical_stats for use in daily processing
    historical_stats <- combined_stats

    # Generate backfill CSV for new locations
    message("\n=== Generating backfill CSV for new locations ===\n")

    # For each date in new_baseline, generate a CSV row with statistics
    backfill_rows <- new_baseline |>
      left_join(locations, by = "location_id") |>
      mutate(
        data_month = month(date),
        data_day = day(date)
      ) |>
      left_join(
        new_stats |> select(location_id, month, day, min, max, p10, p25, p50, p75, p90, mean),
        by = c("location_id", "data_month" = "month", "data_day" = "day")
      ) |>
      mutate(
        pct_median  = value / p50,
        pct_average = value / mean,
        pct_full    = value / capacity
      ) |>
      transmute(
        SiteName       = label_popup,
        Lat            = latitude,
        Lon            = longitude,
        State          = state,
        DoiRegion      = doi_region,
        Huc6           = huc6,
        DataUnits      = unit,
        DataValue      = value,
        DataDate       = format(date, "%m/%d/%Y"),
        DateQueried    = format(Sys.Date(), "%m/%d/%Y"),
        DataDateMax    = max,
        DataDateP90    = p90,
        DataDateP75    = p75,
        DataDateP50    = p50,
        DataDateP25    = p25,
        DataDateP10    = p10,
        DataDateMin    = min,
        DataDateAvg    = mean,
        DataValuePctMdn = pct_median,
        DataValuePctAvg = pct_average,
        StatsPeriod    = STATS_PERIOD,
        MaxCapacity    = capacity,
        ActiveCapacity = active_capacity,
        PctFull        = pct_full,
        TeacupUrl      = NA_character_,
        DataUrl        = NA_character_,
        Comment        = "backfill"
      )

    backfill_filename <- sprintf("backfill_%s.csv", format(Sys.Date(), "%Y%m%d"))
    backfill_path <- file.path(HYDROSHARE_DIR, backfill_filename)
    write_csv(backfill_rows, backfill_path, na = "")

    message(sprintf("Backfill CSV written to: %s", backfill_path))
    message(sprintf("  Contains %d rows for %d new location(s)",
                    nrow(backfill_rows), n_distinct(backfill_rows$SiteName)))

    # Upload backfill to HydroShare (will happen later with main upload)
    BACKFILL_PATH <- backfill_path
    PARQUET_UPDATED <- TRUE  # Flag to upload parquet files at end
  } else {
    message("\nNo historical data retrieved for new locations")
    BACKFILL_PATH <- NULL
    PARQUET_UPDATED <- FALSE
  }
} else {
  message("\nNo new locations detected")
  BACKFILL_PATH <- NULL
  PARQUET_UPDATED <- FALSE
}

# Reload statistics after potential updates
historical_stats <- read_parquet(file.path(OUTPUT_DIR, "historical_statistics.parquet"))

# Track which locations have historical statistics (for logging)
locations_with_stats <- unique(historical_stats$location_id)
n_with_stats <- sum(locations$location_id %in% locations_with_stats)
message(sprintf("  %d locations have historical statistics", n_with_stats))
message(sprintf("  %d locations will have NA for historical metrics",
                nrow(locations) - n_with_stats))

################################################################################
# DATA FETCHING FUNCTIONS
################################################################################

#' Fetch from RISE via WWDH API
#' @param data_type "Storage" or "Elevation" - if Elevation, converts to storage using curve
#' Returns list(value, date, unit, url)
fetch_rise <- function(location_id, target_date, lookback_days = LOOKBACK_DAYS,
                       data_type = "Storage") {
  for (days_back in 0:lookback_days) {
    query_date <- target_date - days_back
    start_date <- query_date
    end_date <- query_date + 1

    url <- paste0(
      WWDH_API_BASE,
      "/collections/rise-edr/locations/", location_id,
      "?parameter-name=Storage",
      "&limit=10",
      "&datetime=", start_date, "/", end_date,
      "&f=csv"
    )

    tryCatch({
      response <- request(url) |>
        req_timeout(60) |>
        req_retry(max_tries = 2, backoff = ~ 2) |>
        req_perform()

      if (resp_status(response) != 200) next

      csv_content <- resp_body_string(response)
      if (nchar(csv_content) < 30) next

      data <- read_csv(I(csv_content), show_col_types = FALSE)
      if (nrow(data) == 0) next

      raw_value <- data$value[1]
      raw_date <- as.Date(data$datetime[1])
      raw_unit <- data$unit[1]

      # Convert elevation to storage if needed
      if (tolower(data_type) == "elevation") {
        storage_val <- elevation_to_storage(location_id, raw_value)
        if (!is.na(storage_val)) {
          message(sprintf("    Converted elevation %.2f ft -> storage %.0f af", raw_value, storage_val))
          return(list(value = storage_val, date = raw_date, unit = "af", url = url))
        } else {
          message(sprintf("    WARNING: Could not convert elevation %.2f ft to storage (no curve)", raw_value))
          return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = url))
        }
      }

      return(list(
        value = raw_value,
        date  = raw_date,
        unit  = raw_unit,
        url   = url
      ))
    }, error = function(e) {
      # Continue to next day
    })
  }
  return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = NA_character_))
}

#' Fetch from USACE CDA API
#' The location_id is "provider/ts_name" (e.g., "spa/Abiquiu.Stor.Inst.15Minutes.0.DCP-rev")
#' @param data_type "Storage" or "Elevation" - if Elevation, converts to storage using curve
#' Returns list(value, date, unit, url)
fetch_usace <- function(location_id, target_date, lookback_days = LOOKBACK_DAYS,
                        data_type = "Storage") {
  # Parse provider and timeseries name from identifier format: "provider/ts_name"
  id_str <- as.character(location_id)
  slash_pos <- str_locate(id_str, "/")[1, "start"]

  if (is.na(slash_pos)) {
    message(sprintf("    USACE: invalid identifier format '%s' (expected 'provider/ts_name')", id_str))
    return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = NA_character_))
  }

  provider <- str_sub(id_str, 1, slash_pos - 1)
  ts_name  <- str_sub(id_str, slash_pos + 1)

  # Query the full lookback window in one call
  begin_date <- target_date - lookback_days
  end_date   <- target_date + 1
  begin_str  <- paste0(format(begin_date, "%Y-%m-%dT00:00:00"), ".000Z")
  end_str    <- paste0(format(end_date, "%Y-%m-%dT00:00:00"), ".000Z")

  url <- sprintf(
    "https://water.usace.army.mil/cda/reporting/providers/%s/timeseries?name=%s&begin=%s&end=%s&format=csv",
    provider,
    URLencode(ts_name, reserved = TRUE),
    begin_str, end_str
  )

  tryCatch({
    response <- request(url) |>
      req_timeout(60) |>
      req_retry(max_tries = 2, backoff = ~ 2) |>
      req_perform()

    body <- resp_body_string(response)

    # USACE CDA CSV has ## comment lines, then datetime,value rows
    # Strip \r from \r\n line endings
    body <- str_replace_all(body, "\r", "")
    lines <- str_split(body, "\n")[[1]]
    data_lines <- lines[!str_starts(lines, "##") & nchar(trimws(lines)) > 0]

    if (length(data_lines) == 0) {
      return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = url))
    }

    # Parse: each line is "datetime,value"
    parsed <- tibble(raw = data_lines) |>
      mutate(
        datetime = str_extract(raw, "^[^,]+"),
        value    = as.numeric(str_extract(raw, "[^,]+$")),
        date     = as.Date(str_sub(datetime, 1, 10))
      ) |>
      filter(!is.na(value)) |>
      arrange(desc(date))

    if (nrow(parsed) == 0) {
      return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = url))
    }

    # Find the most recent value on or before target_date
    recent <- parsed |> filter(date <= target_date)
    if (nrow(recent) == 0) recent <- parsed

    # Extract unit from ## comment lines
    unit_line <- lines[str_starts(lines, "##unit:")]
    unit_val <- if (length(unit_line) > 0) {
      trimws(str_remove(unit_line[1], "##unit:"))
    } else {
      "ac-ft"
    }

    raw_value <- recent$value[1]
    raw_date <- recent$date[1]

    # Convert elevation to storage if needed
    if (tolower(data_type) == "elevation") {
      storage_val <- elevation_to_storage(location_id, raw_value)
      if (!is.na(storage_val)) {
        message(sprintf("    Converted elevation %.2f ft -> storage %.0f af", raw_value, storage_val))
        return(list(value = storage_val, date = raw_date, unit = "af", url = url))
      } else {
        message(sprintf("    WARNING: Could not convert elevation %.2f ft to storage (no curve)", raw_value))
        return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = url))
      }
    }

    return(list(
      value = raw_value,
      date  = raw_date,
      unit  = unit_val,
      url   = url
    ))
  }, error = function(e) {
    message(sprintf("    USACE fetch error: %s", conditionMessage(e)))
    return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = url))
  })
}

#' Fetch from USGS Water Data OGC API (daily values)
#' Parameter 00054 = reservoir storage (acre-feet)
#' Parameter 62614 = lake/reservoir water surface elevation (ft NGVD29)
#' Parameter 62615 = lake/reservoir water surface elevation (ft NAVD88)
#' Parameter 72275 = lake/reservoir elevation (ft USBR datum, e.g. Klamath Basin)
#' Replaces legacy NWIS waterservices.usgs.gov (retiring Q1 2027)
#' The location_id IS the USGS site number (from geojson Identifier)
#' For elevation data, tries 62614 first, then falls back to 72275, then 62615
#' @param data_type "Storage" or "Elevation" - if Elevation, converts to storage
#' Returns list(value, date, unit, url)
fetch_usgs <- function(location_id, target_date, lookback_days = LOOKBACK_DAYS,
                       data_type = "Storage") {
  # location_id is the USGS site number directly from geojson Identifier
  site_no <- as.character(location_id)

  start_date <- target_date - lookback_days
  end_date   <- target_date

  # Select parameter code based on data type
  if (tolower(data_type) == "elevation") {
    # Try elevation parameters: 62614 (NGVD29) or 62615 (NAVD88)
    param_code <- "62614"  # Primary elevation parameter
  } else {
    param_code <- "00054"  # Storage in acre-feet
  }

  url <- sprintf(
    "https://api.waterdata.usgs.gov/ogcapi/v0/collections/daily/items?f=json&monitoring_location_id=USGS-%s&parameter_code=%s&time=%s/%s&limit=50",
    site_no, param_code, start_date, end_date
  )

  tryCatch({
    response <- request(url) |>
      req_timeout(60) |>
      req_retry(max_tries = 2, backoff = ~ 2) |>
      req_perform()

    body <- resp_body_string(response)
    data <- jsonlite::fromJSON(body, simplifyVector = FALSE)

    features <- data$features

    # If elevation query returned nothing, try alternate elevation parameters
    # Priority: 72275 (USBR datum, e.g. Klamath Basin) > 62615 (NAVD88) > 62614 (NGVD29)
    if (length(features) == 0 && tolower(data_type) == "elevation") {
      for (alt_param in c("72275", "62615")) {
        url <- sprintf(
          "https://api.waterdata.usgs.gov/ogcapi/v0/collections/daily/items?f=json&monitoring_location_id=USGS-%s&parameter_code=%s&time=%s/%s&limit=50",
          site_no, alt_param, start_date, end_date
        )
        response <- request(url) |>
          req_timeout(60) |>
          req_retry(max_tries = 2, backoff = ~ 2) |>
          req_perform()
        body <- resp_body_string(response)
        data <- jsonlite::fromJSON(body, simplifyVector = FALSE)
        features <- data$features
        if (length(features) > 0) break
      }
    }

    if (length(features) == 0) {
      return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = url))
    }

    # Parse features into a tibble
    parsed <- tibble(
      date  = as.Date(sapply(features, function(f) f$properties$time)),
      value = as.numeric(sapply(features, function(f) f$properties$value)),
      unit  = sapply(features, function(f) f$properties$unit_of_measure)
    ) |>
      filter(!is.na(value)) |>
      arrange(desc(date))

    if (nrow(parsed) == 0) {
      return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = url))
    }

    raw_value <- parsed$value[1]
    raw_unit <- tolower(parsed$unit[1])

    # Convert elevation to storage if needed
    if (tolower(data_type) == "elevation") {
      storage_val <- elevation_to_storage(location_id, raw_value)
      if (!is.na(storage_val)) {
        message(sprintf("    Converted elevation %.2f ft -> storage %.0f af", raw_value, storage_val))
        return(list(
          value = storage_val,
          date  = parsed$date[1],
          unit  = "af",
          url   = url
        ))
      } else {
        message(sprintf("    WARNING: Could not convert elevation %.2f ft to storage (no curve)", raw_value))
        return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = url))
      }
    }

    # Normalize unit string for storage
    unit_val <- raw_unit
    if (unit_val == "acre-ft") unit_val <- "af"

    return(list(
      value = raw_value,
      date  = parsed$date[1],
      unit  = unit_val,
      url   = url
    ))
  }, error = function(e) {
    message(sprintf("    USGS OGC API error: %s", conditionMessage(e)))
    return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = url))
  })
}

#' Fetch from CDEC API
#' Sensor 15 = reservoir storage
#' The location_id IS the CDEC station code (from geojson Identifier)
#' @param data_type "Storage" or "Elevation" - if Elevation, converts to storage using curve
#' Returns list(value, date, unit, url)
fetch_cdec <- function(location_id, target_date, lookback_days = LOOKBACK_DAYS,
                       data_type = "Storage") {
  # location_id is the CDEC station code directly from geojson Identifier
  station <- as.character(location_id)

  start_date <- target_date - lookback_days
  end_date   <- target_date

  url <- sprintf(
    "https://cdec.water.ca.gov/dynamicapp/req/CSVDataServlet?Stations=%s&SensorNums=15&dur_code=D&Start=%s&End=%s",
    station, start_date, end_date
  )

  tryCatch({
    response <- request(url) |>
      req_timeout(60) |>
      req_retry(max_tries = 2, backoff = ~ 2) |>
      req_perform()

    body <- resp_body_string(response)
    data <- read_csv(I(body), show_col_types = FALSE,
                     col_types = cols(`DATE TIME` = col_character(),
                                     `OBS DATE` = col_character(),
                                     .default = col_guess()))

    if (nrow(data) == 0 || !"VALUE" %in% names(data)) {
      return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = url))
    }

    data <- data |>
      mutate(date  = as.Date(str_sub(`DATE TIME`, 1, 8), format = "%Y%m%d"),
             value = as.numeric(VALUE)) |>
      filter(!is.na(value)) |>
      arrange(desc(date))

    if (nrow(data) == 0) {
      return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = url))
    }

    # CDEC UNITS column
    unit_val <- if ("UNITS" %in% names(data)) data$UNITS[1] else "AF"
    raw_value <- data$value[1]
    raw_date <- data$date[1]

    # Convert elevation to storage if needed
    if (tolower(data_type) == "elevation") {
      storage_val <- elevation_to_storage(location_id, raw_value)
      if (!is.na(storage_val)) {
        message(sprintf("    Converted elevation %.2f ft -> storage %.0f af", raw_value, storage_val))
        return(list(value = storage_val, date = raw_date, unit = "af", url = url))
      } else {
        message(sprintf("    WARNING: Could not convert elevation %.2f ft to storage (no curve)", raw_value))
        return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = url))
      }
    }

    return(list(
      value = raw_value,
      date  = raw_date,
      unit  = tolower(unit_val),
      url   = url
    ))
  }, error = function(e) {
    return(list(value = NA, date = as.Date(NA), unit = NA_character_, url = url))
  })
}

#' Master fetch: dispatch to the correct source-specific function
#' @param data_type "Storage" or "Elevation" - passed to source-specific fetchers
#' Returns list(value, date, unit, url)
fetch_current_value <- function(location_id, source_str, target_date,
                                lookback_days = LOOKBACK_DAYS,
                                data_type = "Storage") {
  src_type <- classify_source(source_str)

  result <- switch(src_type,
    "rise"     = fetch_rise(location_id, target_date, lookback_days, data_type),
    "usace_cda" = fetch_usace(location_id, target_date, lookback_days, data_type),
    "usgs"     = fetch_usgs(location_id, target_date, lookback_days, data_type),
    "cdec"     = fetch_cdec(location_id, target_date, lookback_days, data_type),
    # For "unknown" or RISE (Pending), still try RISE
    fetch_rise(location_id, target_date, lookback_days, data_type)
  )

  return(result)
}

################################################################################
# MAIN PROCESSING LOOP
################################################################################

message("\n=== Fetching current values ===\n")

results <- list()

for (i in seq_len(nrow(locations))) {
  loc <- locations[i, ]
  location_id   <- loc$location_id
  location_name <- loc$name
  source_str    <- loc$source
  data_type_str <- if (!is.na(loc$data_type)) loc$data_type else "Storage"

  src_type <- classify_source(source_str)
  type_suffix <- if (tolower(data_type_str) == "elevation") " (elevation->storage)" else ""
  message(sprintf("[%d/%d] %s (ID: %s) [%s]%s...",
                  i, nrow(locations), location_name, location_id, src_type, type_suffix))

  # Fetch current value from the appropriate source
  current <- fetch_current_value(location_id, source_str, TARGET_DATE,
                                  data_type = data_type_str)

  if (is.na(current$value)) {
    message(sprintf("  No data found"))
    results[[i]] <- tibble(
      location_id = location_id,
      name        = location_name,
      data_value  = NA_real_,
      data_date   = as.Date(NA),
      data_unit   = NA_character_,
      data_url    = if (!is.null(current$url)) current$url else NA_character_
    )
    next
  }

  message(sprintf("  Value: %s %s (date: %s)",
                  format(current$value, big.mark = ","),
                  current$unit,
                  current$date))

  results[[i]] <- tibble(
    location_id = location_id,
    name        = location_name,
    data_value  = current$value,
    data_date   = current$date,
    data_unit   = current$unit,
    data_url    = current$url
  )

  # Rate limiting
  Sys.sleep(0.25)
}

# Combine results
current_data <- bind_rows(results)

################################################################################
# JOIN WITH HISTORICAL STATISTICS
################################################################################

message("\n=== Joining with historical statistics ===\n")

# Get the month and day for the target date to look up correct historical stats
target_month <- month(TARGET_DATE)
target_day   <- day(TARGET_DATE)

# Get historical stats for today's calendar day
todays_stats <- historical_stats |>
  filter(month == target_month, day == target_day) |>
  select(location_id, min, max, p10, p25, p50, p75, p90, mean, unit)

# Join current data with location metadata and historical stats
# Use left_join so ALL locations appear even without stats
output_data <- current_data |>
  left_join(locations, by = c("location_id", "name")) |>
  left_join(todays_stats, by = "location_id", suffix = c("", "_hist")) |>
  mutate(
    # Calculate derived values (will be NA if stats or current value missing)
    pct_median  = data_value / p50,
    pct_average = data_value / mean,
    pct_full    = data_value / capacity,
    # Format dates
    data_date_fmt = format(data_date, "%m/%d/%Y"),
    date_queried  = format(Sys.Date(), "%m/%d/%Y")
  )

################################################################################
# GENERATE OUTPUT CSV
################################################################################

message("\n=== Generating output CSV ===\n")

output_csv <- output_data |>
  transmute(
    SiteName       = label_popup,
    Lat            = latitude,
    Lon            = longitude,
    State          = state,
    DoiRegion      = doi_region,
    Huc6           = huc6,
    DataUnits      = coalesce(data_unit, unit),
    DataValue      = data_value,
    DataDate       = data_date_fmt,
    DateQueried    = date_queried,
    DataDateMax    = max,
    DataDateP90    = p90,
    DataDateP75    = p75,
    DataDateP50    = p50,
    DataDateP25    = p25,
    DataDateP10    = p10,
    DataDateMin    = min,
    DataDateAvg    = mean,
    DataValuePctMdn = pct_median,
    DataValuePctAvg = pct_average,
    StatsPeriod    = STATS_PERIOD,
    MaxCapacity    = capacity,
    ActiveCapacity = active_capacity,
    PctFull        = pct_full,
    TeacupUrl      = NA_character_,
    DataUrl        = data_url,
    Comment        = NA_character_
  )

# Generate filename and write to hydroshare directory (git-ignored, uploaded to HS)
output_filename <- sprintf("droughtData%s.csv", format(TARGET_DATE, "%Y%m%d"))
if (!dir.exists(HYDROSHARE_DIR)) dir.create(HYDROSHARE_DIR, recursive = TRUE)
output_path <- file.path(HYDROSHARE_DIR, output_filename)

# Write CSV (standard comma-separated)
write_csv(output_csv, output_path, na = "")

message(sprintf("Output written to: %s", output_path))
message(sprintf("  Total locations: %d", nrow(output_csv)))
message(sprintf("  With data: %d", sum(!is.na(output_csv$DataValue))))
message(sprintf("  Missing data: %d", sum(is.na(output_csv$DataValue))))
message(sprintf("  With historical stats: %d", sum(!is.na(output_csv$DataDateP50))))
message(sprintf("  Without historical stats: %d", sum(is.na(output_csv$DataDateP50))))

# Source type breakdown
source_counts <- locations |>
  mutate(src_type = sapply(source, classify_source)) |>
  count(src_type)
message("\n  Source breakdown:")
for (j in seq_len(nrow(source_counts))) {
  message(sprintf("    %s: %d", source_counts$src_type[j], source_counts$n[j]))
}

################################################################################
# UPLOAD TO HYDROSHARE
################################################################################

message("\n=== Uploading to HydroShare ===\n")

hs_username <- Sys.getenv("HYDROSHARE_USERNAME", unset = "")
hs_password <- Sys.getenv("HYDROSHARE_PASSWORD", unset = "")

if (hs_username == "" || hs_password == "") {
  message("WARNING: HYDROSHARE_USERNAME and/or HYDROSHARE_PASSWORD not set.")
  message("Skipping HydroShare upload. Set environment variables to enable upload.")
} else {

  upload_to_hydroshare <- function(file_path, resource_id, username, password) {
    filename <- basename(file_path)

    # First, try to delete the existing file (if updating daily, old file may exist)
    delete_url <- sprintf("%s/hsapi/resource/%s/files/%s/",
                          HYDROSHARE_BASE_URL, resource_id, filename)

    tryCatch({
      request(delete_url) |>
        req_auth_basic(username, password) |>
        req_method("DELETE") |>
        req_timeout(60) |>
        req_perform()
      message(sprintf("  Deleted existing file: %s", filename))
    }, error = function(e) {
      message(sprintf("  No existing file to delete (or delete failed): %s", filename))
    })

    # Upload the new file
    upload_url <- sprintf("%s/hsapi/resource/%s/files/",
                          HYDROSHARE_BASE_URL, resource_id)

    response <- request(upload_url) |>
      req_auth_basic(username, password) |>
      req_body_multipart(file = curl::form_file(file_path)) |>
      req_timeout(120) |>
      req_perform()

    status <- resp_status(response)
    if (status >= 200 && status < 300) {
      message(sprintf("  Successfully uploaded %s to HydroShare resource %s",
                      filename, resource_id))
    } else {
      warning(sprintf("  Upload returned status %d", status))
    }

    return(status)
  }

  tryCatch({
    upload_to_hydroshare(output_path, HYDROSHARE_RESOURCE_ID, hs_username, hs_password)
  }, error = function(e) {
    message(sprintf("ERROR uploading to HydroShare: %s", e$message))
  })

  # Upload backfill file if it exists
  if (exists("BACKFILL_PATH") && !is.null(BACKFILL_PATH) && file.exists(BACKFILL_PATH)) {
    message("\nUploading backfill file to HydroShare...")
    tryCatch({
      upload_to_hydroshare(BACKFILL_PATH, HYDROSHARE_RESOURCE_ID, hs_username, hs_password)
    }, error = function(e) {
      message(sprintf("ERROR uploading backfill to HydroShare: %s", e$message))
    })
  }

  # Upload updated parquet files if backfill occurred
  # This ensures the next run starts with the latest historical data
  if (exists("PARQUET_UPDATED") && PARQUET_UPDATED) {
    message("\n=== Uploading updated parquet files to HydroShare ===")

    stats_file <- file.path(OUTPUT_DIR, "historical_statistics.parquet")
    baseline_file <- file.path(OUTPUT_DIR, "historical_baseline.parquet")

    if (file.exists(stats_file)) {
      message(sprintf("Uploading historical_statistics.parquet (%.1f MB)...",
                      file.size(stats_file) / 1e6))
      tryCatch({
        upload_to_hydroshare(stats_file, HYDROSHARE_RESOURCE_ID, hs_username, hs_password)
      }, error = function(e) {
        message(sprintf("ERROR uploading historical_statistics.parquet: %s", e$message))
      })
    }

    if (file.exists(baseline_file)) {
      message(sprintf("Uploading historical_baseline.parquet (%.1f MB)...",
                      file.size(baseline_file) / 1e6))
      tryCatch({
        upload_to_hydroshare(baseline_file, HYDROSHARE_RESOURCE_ID, hs_username, hs_password)
      }, error = function(e) {
        message(sprintf("ERROR uploading historical_baseline.parquet: %s", e$message))
      })
    }

    message("Parquet files uploaded - next run will use updated historical data")
  }
}

################################################################################
# SUMMARY
################################################################################

message("\n=== Summary ===")
message(sprintf("Target date: %s", TARGET_DATE))
message(sprintf("Locations processed: %d", nrow(output_csv)))
message(sprintf("Locations with current data: %d (%.1f%%)",
                sum(!is.na(output_csv$DataValue)),
                100 * sum(!is.na(output_csv$DataValue)) / nrow(output_csv)))
message(sprintf("Locations with historical stats: %d (%.1f%%)",
                sum(!is.na(output_csv$DataDateP50)),
                100 * sum(!is.na(output_csv$DataDateP50)) / nrow(output_csv)))
message(sprintf("Output file: %s", output_path))
message(sprintf("Completed at: %s", Sys.time()))
